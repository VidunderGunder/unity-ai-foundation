default_settings:
  trainer_type: ppo
  summary_freq: 50000
  time_horizon: 64
  max_steps: 500000
  keep_checkpoints: 5
  checkpoint_interval: 500000
  threaded: true
  hyperparameters:
    learning_rate: 0.0003
    batch_size: 256
    buffer_size: 10240
    learning_rate_schedule: linear
    beta: 0.005
    epsilon: 0.2
    lambd: 0.95
    num_epoch: 3
  network_settings:
    hidden_units: 128
    num_layers: 2
    normalize: false
    vis_encode_type: simple
    memory:
  reward_signals:
    strength: 1
    num_layers: 0.99
    learning_rate: 0.0003
    extrinsic:
      strength: 1
      gamma: 0.99
    curiosity:
      strength: 1
      gamma: 0.99
      learning_rate: 0.0003
      network_settings:
        hidden_units: 128
        num_layers: 2
        normalize: false
        vis_encode_type: simple
    gail:
      strength: 1
      gamma: 0.99
      learning_rate: 0.0003
      use_actions: false
      use_vail: false
      network_settings:
        hidden_units: 128
        num_layers: 2
        normalize: false
        vis_encode_type: simple
    rnd:
      strength: 1
      gamma: 0.99
      learning_rate: 0.0003
      network_settings:
        hidden_units: 128
        num_layers: 2
        normalize: false
        vis_encode_type: simple
  behavior_cloning:
    strength: 1
    steps: 0
    num_epoch: 3
    samples_per_update: 0
  self_play:
    save_steps: 20000
    team_change: 100000
    swap_steps: 10000
    num_epoch: 3
    samples_per_update: 10
checkpoint_settings:
  run_id: mlady
  load_model: false
  resume: false
  force: false
  train_model: false
  inference: false
  results_dir: Results
engine_settings:
  time_scale: 1
  quality_level: 5
  target_frame_rate: -1
  capture_frame_rate: 60
  no_graphics: false
  width: 84
  height: 84
torch_settings:
  device: cuda
debug: false
